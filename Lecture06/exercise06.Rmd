---
title: 'Data Quality: Lecture 6'
author: "Philipp Drebes"
date: "`r format(Sys.Date(), format='%d.%m.%Y')`"
output:
  pdf_document: 
    keep_tex: yes
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require("readxl")) install.packages("readxl", dependencies=TRUE)
if (!require("MVA")) install.packages("MVA", dependencies=TRUE)
if (!require("cluster")) install.packages("cluster", dependencies=TRUE)
```

## Task 01

A data set *learning* is given. The dataset *learning* shows the relationship between learning effort in self-study [hours per week] and success on the final exam [index 0 to 10] in a master's program.

Run a "Cluster Based" outlier detection applying k-means clustering after Barai & Dey (2017). Calculate solutions with two different numbers of clusters. x = 3 and x = 4 in kmeans(datas, centers = x, nstart = 10)

How can the solutions be interpreted?

### Load & Explore

```{r}
library(readxl)
library(MVA)
library(cluster)

data <- read_excel("data/learning.xlsx")
any(is.na(data))
summary(data)
plot(data)
```

### Preparation

```{r}
data.scaled <- scale(data, center = FALSE, scale = TRUE)
dists <- dist(data.scaled)
summary(data.scaled)
boxplot(data.scaled)
plot(data.scaled)
```

### Clustering

#### Scree plot

```{r}
reps <- rep(0, 6)
for (i in 1:6) reps[i] <- sum(kmeans(data.scaled, centers = i, nstart = 20)$withinss)
plot(1:6, reps, type = "b", xlab = "Number of groups", ylab = "Sum of squares")
```

#### 4 Clusters

```{r}
km.4 <- kmeans(data.scaled, centers = 4, nstart = 10)
km.4.groups <- km.4$cluster
km.4.groups

cluster.size.4 <- cbind(sum(km.4.groups == 1), sum(km.4.groups == 2), 
                        sum(km.4.groups == 3), sum(km.4.groups == 4))
cluster.size.4
```

##### Plot / Silhouette plot

```{r}
plot(data.scaled, pch = km.4.groups, col=km.4.groups, lwd=2)
legend("topleft", legend = 1:4, pch = 1:4, col=1:4, bty="n")

plot(silhouette(km.4.groups, dists))
```

#### 3 Clusters

```{r}
km.3 <- kmeans(data.scaled, centers = 3, nstart = 10)
km.3.groups <- km.3$cluster
km.3.groups

cluster.size.3 <- cbind(sum(km.3.groups == 1), sum(km.3.groups == 2), 
                        sum(km.3.groups == 3))
cluster.size.3
```

##### Plot / Silhouette plot

```{r}
plot(data.scaled, pch = km.3.groups, col=km.3.groups, lwd=2)
legend("topleft", legend = 1:3, pch = 1:3, col=1:3, bty="n")

plot(silhouette(km.3.groups, dists))
```

#### Hierarchical cluster analysis

```{r}
dc <- dist(data.scaled, method = "euclidean")
dc

cc <- hclust(dc, method = "complete")
plot(cc,cex = 0.3, hang = -1)
```

### Interpretation

According to the numbers you could argue for 3 and 4 clusters. However, 4 clusters seems to be more appropriate considering the domain of the data set.

1.  Those who learn little and have a bad grade

2.  Those who learn an average amount and have an average grade

3.  Those who learn a lot and have good performance

4.  Those who learn a lot and still have bad performance (maybe not an appropriate learning technique)
